# Lecture 08: Sparse matrices and stopping criteria

##  Sparse Matrices

There are two main ways in which sparse matrices can be exploited in order to obtain benefits within iterative methods.

-   The storage must be reduced for $O(n^2)$.

-   The cost per iteration must be reduced from $O(n^2)$.

Recall that a sparse matrix is defined to be such that it has at most $\alpha n$ non-zero entries (where $\alpha$ is independent of $n$).

-   Typically this happens when we know there are at most $\alpha$ non-zero entries in any row.

### Sparse matrix storage

The simplest way in which a sparse matrix is stored is using three arrays:

-   an array of floating point numbers (`A_real` say) that stores the non-zero entries;

-   an array of integers (`I_row` say) that stores the row number of the corresponding entry in the real array;

-   an array of integers (`I_col` say) that stores the column numbers of the corresponding entry in the real array.

This requires just $3 \alpha n$ units of storage - i.e.Â $O(n)$.

### Sparse matrix multiplication

Given the above storage pattern, the following algorithm will execute a sparse matrix-vector multiplication ($\vec{z} = A \vec{y}$) in $O(n)$ operations:

``` python
z = np.zeros((n, 1))
for k in range(nonzero):
    z[I_row[k]] = z[I_row[k]] + A_real[k] * y[I_col[k]]
```

-   Here `nonzero` is the number of non-zero entries in the matrix.

-   Note that the cost of this operation is $O(n)$ as required.

## Convergence of an iterative method

We have discussed the construction of **iterations** which aim to find the solution of the equations $A \vec{x} = \vec{b}$ through a sequence of better and better approximations $\vec{x}^{(k)}$.

-   In general the iteration takes the form

    $$
    \vec{x}^{(k+1)} = \vec{F}(\vec{x}^{(k)})
    $$

	where $\vec{x}^{(k)}$ is a vector of values and $\vec{F}$ is some vector-valued function which we have defined.

-   How can we decide if this iteration has converged?

    -   We need $\vec{x} - \vec{x}^{(k)}$ to be small.

### Magnitude of a vector

How do we decide that a vector/array is small?

-   We need a way of defining the size/magnitude of an array.

-   The most common measure is to use the "Euclidean norm" of an array.

-   This is defined to be the square root of the sum of squares of the entries of the array:

    $$
    \| \vec{r} \| = \sqrt{ \sum_{i=1}^n r_i^2 }
    $$

	where $\vec{r}$ is a vector with $n$ entries.

### Examples

Consider the following sequence $\vec{x}^{(k)}$:

$$
\begin{pmatrix}
1 \\ -1
\end{pmatrix},
\begin{pmatrix}
1.5 \\ 0.5
\end{pmatrix},
\begin{pmatrix}
1.75 \\ 0.25
\end{pmatrix},
\begin{pmatrix}
1.875 \\ 0.125
\end{pmatrix},
\begin{pmatrix}
1.9375 \\ -0.0625
\end{pmatrix},
\begin{pmatrix}
1.96875 \\ -0.03125
\end{pmatrix},
\ldots
$$

-   What is $\|\vec{x}^{(1)} - \vec{x}^{(0)}\|$?
-   What is $\|\vec{x}^{(5)} - \vec{x}^{(4)}\|$?

Let $\vec{x} = \begin{pmatrix} 2 \\ 0 \end{pmatrix}$.

-   What is $\|\vec{x} - \vec{x}^{(3)}\|$?
-   What is $\|\vec{x} - \vec{x}^{(4)}\|$?
-   What is $\|\vec{x} - \vec{x}^{(5)}\|$?

### Convergence detection

Rather than decide in advance how many iterations (of the Jacobi or Gauss-Seidel methods) to use stopping criteria:

-   This could be a maximum number of iterations.

-   This could be the *change* in values is small enough:

    $$
    \|x^{(k+1)} - \vec{x}^{(k)}\| < tol,
    $$


-   This could be the *norm of the residual* is small enough:

	$$
	\| \vec{r} \| = \| \vec{b} - A \vec{x}^{(k)} \| < tol
	$$

-   In both cases, we call $tol$ the **convergence tolerance**.

-   The choice of $tol$ will control the accuracy of the solution.

### Discussion

> What is a good convergence tolerance?

## Failure to converge

In general there are two possible reasons that an iteration may fail to converge.

-   It may **diverge** - this means that $\|\vec{x}^{(k)}\| \to \infty$ as $k$ (the number of iterations) increases, e.g.:

    $$
    \begin{pmatrix}
    1 \\ 1
    \end{pmatrix},
    \begin{pmatrix}
    4 \\ 2
    \end{pmatrix},
    \begin{pmatrix}
    16 \\ 4
    \end{pmatrix},
    \begin{pmatrix}
    64 \\ 8
    \end{pmatrix},
    \begin{pmatrix}
    256 \\ 16
    \end{pmatrix},
    \begin{pmatrix}
    1024 \\ 32
    \end{pmatrix},
    \ldots
    $$

-   It may *neither* converge nor diverge, e.g.:

    $$
    \begin{pmatrix}
    1 \\ 1
    \end{pmatrix},
    \begin{pmatrix}
    2 \\ 0
    \end{pmatrix},
    \begin{pmatrix}
    3 \\ 1
    \end{pmatrix},
    \begin{pmatrix}
    1 \\ 0
    \end{pmatrix},
    \begin{pmatrix}
    2 \\ 1
    \end{pmatrix},
    \begin{pmatrix}
    3 \\ 0
    \end{pmatrix},
    \ldots
    $$

### Implementation

In addition to testing for convergence it is also necessary to include tests for failure to converge.

-   Divergence may be detected by monitoring $\|\vec{x}^{(k)}\|$.

-   Impose a maximum number of iterations to ensure that the loop is not repeated forever!


## Further reading

- Wikipedia [Sparse matrix](https://en.wikipedia.org/wiki/Sparse_matrix) - including a long detailed list of software libraries support sparse matrices.

- [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html) custom routines specialised to sparse matrices
- [SuiteSparse](http://faculty.cse.tamu.edu/davis/suitesparse.html), a suite of sparse matrix algorithms, geared toward the direct solution of sparse linear systems

- Stackoverflow: [Using a sparse matrix vs numpy array](https://stackoverflow.com/questions/36969886/using-a-sparse-matrix-versus-numpy-array)
- Jason Brownlee: [A gentle introduction to sparse matrices for machine learning](https://machinelearningmastery.com/sparse-matrices-for-machine-learning/), Machine learning mastery


- Jack Dongarra [Templates for the solution of linear systems: Stopping criteria](http://www.netlib.org/linalg/html_templates/node83.html)
