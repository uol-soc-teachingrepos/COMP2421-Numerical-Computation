{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23623efe",
   "metadata": {},
   "source": [
    "# Lecture 18: Robust nonlinear solvers\n",
    "\n",
    "### Recap\n",
    "\n",
    "-   In the previous lecture we consider a modified version of Newton's method in which $f'(x^{(i)})$ is approximated:\n",
    "\n",
    "    $$\n",
    "    f'(x^{(i)}) \\approx \\frac{f(x^{(i)} + \\mathrm{d}x) - f(x^{(i)})}{\\mathrm{d}x}.\n",
    "    $$\n",
    "\n",
    "-   For a suitable choice of $\\mathrm{d}x$ this was shown to work very well, producing results of the same accuracy as Newton's method in almost the same number of iterations.\n",
    "\n",
    "-   We then introduced another popular modification of Newton's method that also approximates $f'(x^{(i)})$ as above for a particular choice of $\\mathrm{d}x = x^{(i-1)} - x^{(i)}$.\n",
    "\n",
    "-   The resulting iteration is known as the **secant method**.\n",
    "\n",
    "## Reliability\n",
    "\n",
    "-   The Newton, modified Newton and secant methods may not always converge for a particular choice of $x^{(0)}$.\n",
    "\n",
    "-   The secant method in particular will fail if $x^{(0)} = x^{(1)}$ or $f(x^{(0)}) = f(x^{(1)})$.\n",
    "\n",
    "-   Each of these methods can break down when\n",
    "\n",
    "    -   $f'(x^{(i)}) = 0$ for $x^{(i)} \\neq x^*$;\n",
    "    -   $f'(x^{(i)})$ is small but nonzero, in which case $x^{(i+1)}$ may be further away from $x^*$ than $x^{(i)}$.\n",
    "\n",
    "-   These methods are guaranteed to converge when $x^{(0)}$ is \"sufficiently close\" to $x^*$.\n",
    "\n",
    "-   In practice a good initial estimate $x^{(0)}$ may not be known in advance.\n",
    "\n",
    "## A combined approach\n",
    "\n",
    "In this algorithm we seek to combine the reliability of the bisection algorithm with the speed of the secant algorithm:\n",
    "\n",
    "0.  Take a function $f(x)$ and initial estimate $x^{(0)}$.\n",
    "\n",
    "1.  Search for an initial point $x^{(1)}$ such that $f(x^{(0)}) f(x^{(1)}) < 0$, i.e. an initial bracket $[x^{(0)}, x^{(1)}]$ for $x^*$.\n",
    "\n",
    "2.  Take a single step with the secant method\n",
    "\n",
    "    $$\n",
    "    x^{(2)} = x^{(1)} - f(x^{(1)}) \\frac{x^{(1)} - x^{(0)}}{f(x^{(1)}) - f(x^{(0)})}\n",
    "    $$\n",
    "\n",
    "\tto produce a new estimate.\n",
    "\n",
    "3.  If $x^{(2)}$ is outside $[x^{(0)}, x^{(1)}]$ then reject it and apply a single bisection step, i.e. find $x^{(2)} = (x^{(0)} + x^{(1)}) / 2$.\n",
    "\n",
    "4.  Update the bracket to\n",
    "\n",
    "    $$\n",
    "    \\begin{cases}\n",
    "    [x^{(0)}, x^{(2)}] & \\text{ if } f(x^{(0)}) f(x^{(2)}) \\le 0; \\\\\n",
    "    [x^{(2)}, x^{(1)}] & \\text{ if } f(x^{(2)}) f(x^{(1)}) \\le 0.\n",
    "    \\end{cases}\n",
    "    $$\n",
    "\n",
    "5.  If the method has not yet converged return to step 3 with the new interval.\n",
    "\n",
    "### Notes\n",
    "\n",
    "-   When the secant iteration becomes unreliable the algorithm reverts to the bisection approach.\n",
    "\n",
    "-   When the approximation is close to the root the secant method will usually be used and should converge (almost) as rapidly as Newton.\n",
    "\n",
    "-   The approach can easily be adapted to find all of the roots in a given interval.\n",
    "\n",
    "-   Variations and other hybrid methods are implemented in `scipy` as [`scipy.optimize.root_scalar`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.root_scalar.html?highlight=root_scalar#scipy.optimize.root_scalar).\n",
    "\n",
    "### Stopping criteria\n",
    "\n",
    "The algorithm stops if any of the following holds:\n",
    "\n",
    "-   ${|x^{(i)} - x^{(i-1)}|}/{|x^{(i)}|} < \\texttt{tolx}$;\n",
    "-   $|f(x^{(i)})| < \\texttt{tolfun}$;\n",
    "-   the number of iterations exceeds a specified number `maxiter`.\n",
    "\n",
    "Criticisms:\n",
    "\n",
    "-   convergence criteria should ideally satisfy *both* ${|x^{(i)} - x^{(i-1)}|}/{|x^{(i)}|} < \\texttt{tolx}$ and $|f(x^{(i)})| < \\texttt{tolfun}$;\n",
    "-   cannot find solutions which do not cross the $x$-axis.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "The method has been slightly improved to become [Brent's method](https://en.wikipedia.org/wiki/Brent%27s_method) which is implemented in `scipy` as [`scipy.optimize.brentq`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.brentq.html).\n",
    "\n",
    "Note that this method requires both initial estimates $x^{(0)}$ and $x^{(1)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a37152d8",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function brentq in module scipy.optimize.zeros:\n",
      "\n",
      "brentq(f, a, b, args=(), xtol=2e-12, rtol=8.881784197001252e-16, maxiter=100, full_output=False, disp=True)\n",
      "    Find a root of a function in a bracketing interval using Brent's method.\n",
      "    \n",
      "    Uses the classic Brent's method to find a zero of the function `f` on\n",
      "    the sign changing interval [a , b]. Generally considered the best of the\n",
      "    rootfinding routines here. It is a safe version of the secant method that\n",
      "    uses inverse quadratic extrapolation. Brent's method combines root\n",
      "    bracketing, interval bisection, and inverse quadratic interpolation. It is\n",
      "    sometimes known as the van Wijngaarden-Dekker-Brent method. Brent (1973)\n",
      "    claims convergence is guaranteed for functions computable within [a,b].\n",
      "    \n",
      "    [Brent1973]_ provides the classic description of the algorithm. Another\n",
      "    description can be found in a recent edition of Numerical Recipes, including\n",
      "    [PressEtal1992]_. A third description is at\n",
      "    http://mathworld.wolfram.com/BrentsMethod.html. It should be easy to\n",
      "    understand the algorithm just by reading our code. Our code diverges a bit\n",
      "    from standard presentations: we choose a different formula for the\n",
      "    extrapolation step.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    f : function\n",
      "        Python function returning a number. The function :math:`f`\n",
      "        must be continuous, and :math:`f(a)` and :math:`f(b)` must\n",
      "        have opposite signs.\n",
      "    a : scalar\n",
      "        One end of the bracketing interval :math:`[a, b]`.\n",
      "    b : scalar\n",
      "        The other end of the bracketing interval :math:`[a, b]`.\n",
      "    xtol : number, optional\n",
      "        The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "        atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "        parameter must be nonnegative. For nice functions, Brent's\n",
      "        method will often satisfy the above condition with ``xtol/2``\n",
      "        and ``rtol/2``. [Brent1973]_\n",
      "    rtol : number, optional\n",
      "        The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "        atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "        parameter cannot be smaller than its default value of\n",
      "        ``4*np.finfo(float).eps``. For nice functions, Brent's\n",
      "        method will often satisfy the above condition with ``xtol/2``\n",
      "        and ``rtol/2``. [Brent1973]_\n",
      "    maxiter : int, optional\n",
      "        If convergence is not achieved in `maxiter` iterations, an error is\n",
      "        raised. Must be >= 0.\n",
      "    args : tuple, optional\n",
      "        Containing extra arguments for the function `f`.\n",
      "        `f` is called by ``apply(f, (x)+args)``.\n",
      "    full_output : bool, optional\n",
      "        If `full_output` is False, the root is returned. If `full_output` is\n",
      "        True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
      "        a `RootResults` object.\n",
      "    disp : bool, optional\n",
      "        If True, raise RuntimeError if the algorithm didn't converge.\n",
      "        Otherwise, the convergence status is recorded in any `RootResults`\n",
      "        return object.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    x0 : float\n",
      "        Zero of `f` between `a` and `b`.\n",
      "    r : `RootResults` (present if ``full_output = True``)\n",
      "        Object containing information about the convergence. In particular,\n",
      "        ``r.converged`` is True if the routine converged.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    `f` must be continuous.  f(a) and f(b) must have opposite signs.\n",
      "    \n",
      "    Related functions fall into several classes:\n",
      "    \n",
      "    multivariate local optimizers\n",
      "      `fmin`, `fmin_powell`, `fmin_cg`, `fmin_bfgs`, `fmin_ncg`\n",
      "    nonlinear least squares minimizer\n",
      "      `leastsq`\n",
      "    constrained multivariate optimizers\n",
      "      `fmin_l_bfgs_b`, `fmin_tnc`, `fmin_cobyla`\n",
      "    global optimizers\n",
      "      `basinhopping`, `brute`, `differential_evolution`\n",
      "    local scalar minimizers\n",
      "      `fminbound`, `brent`, `golden`, `bracket`\n",
      "    N-D root-finding\n",
      "      `fsolve`\n",
      "    1-D root-finding\n",
      "      `brenth`, `ridder`, `bisect`, `newton`\n",
      "    scalar fixed-point finder\n",
      "      `fixed_point`\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [Brent1973]\n",
      "       Brent, R. P.,\n",
      "       *Algorithms for Minimization Without Derivatives*.\n",
      "       Englewood Cliffs, NJ: Prentice-Hall, 1973. Ch. 3-4.\n",
      "    \n",
      "    .. [PressEtal1992]\n",
      "       Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; and Vetterling, W. T.\n",
      "       *Numerical Recipes in FORTRAN: The Art of Scientific Computing*, 2nd ed.\n",
      "       Cambridge, England: Cambridge University Press, pp. 352-355, 1992.\n",
      "       Section 9.3:  \"Van Wijngaarden-Dekker-Brent Method.\"\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> def f(x):\n",
      "    ...     return (x**2 - 1)\n",
      "    \n",
      "    >>> from scipy import optimize\n",
      "    \n",
      "    >>> root = optimize.brentq(f, -2, 0)\n",
      "    >>> root\n",
      "    -1.0\n",
      "    \n",
      "    >>> root = optimize.brentq(f, 0, 2)\n",
      "    >>> root\n",
      "    1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import brentq\n",
    "\n",
    "help(brentq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7cda44",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "How can we estimate this initial bracket from an initial guess?\n",
    "\n",
    "### Examples\n",
    "\n",
    "We will test `scipy`'s implementation with our three examples from before.\n",
    "\n",
    "Consider the example with $f(x) = x^2 - R$ and $R=2$. Take $x^{(0)} = 0$ and $x^{(1)} = 2$ and use the function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "437b138d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.4142133199955025,\n",
       "       converged: True\n",
       "            flag: 'converged'\n",
       "  function_calls: 8\n",
       "      iterations: 7\n",
       "            root: 1.4142133199955025)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sqrt2(x):\n",
    "    return x**2 - 2.0\n",
    "\n",
    "\n",
    "brentq(sqrt2, 0.0, 2.0, xtol=1.0e-4, maxiter=100, full_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03637cb9",
   "metadata": {},
   "source": [
    "-   The algorithm gives $x^* = 1.4142$ after 7 iterations.\n",
    "\n",
    "Consider the example with $f(x) = x^2 - R$ with $R=2$, take $x^{(0)} = 0$ and $x^{(1)} = 2$ and use the function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4569e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.414213562373095,\n",
       "       converged: True\n",
       "            flag: 'converged'\n",
       "  function_calls: 10\n",
       "      iterations: 9\n",
       "            root: 1.414213562373095)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "eps = np.finfo(np.double).eps\n",
    "brentq(sqrt2, 0.0, 2.0, xtol=4 * eps, maxiter=100, full_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd3aaf5",
   "metadata": {},
   "source": [
    "-   The algorithm gives $x^* = 1.414214$ after 9 iterations.\n",
    "-   Convergence is to *machine precision* - so it takes more iterations than previously - but not too many!\n",
    "\n",
    "Consider the compound interest example with $[x^{(0)}, x^{(1)}] = [200, 300]$, using the function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d0d6c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235.87468057187797,\n",
       "       converged: True\n",
       "            flag: 'converged'\n",
       "  function_calls: 6\n",
       "      iterations: 5\n",
       "            root: 235.87468057187797)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compound(n):\n",
    "    # Set P, M and r.\n",
    "    P = 150000\n",
    "    M = 1000\n",
    "    r = 5.0\n",
    "    # Evaluate the function.\n",
    "    i = r / 1200\n",
    "    f = M - P * (i * (1 + i) ** n) / ((1 + i) ** n - 1)\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "brentq(compound, 200, 300, xtol=1.0e-1, full_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80154742",
   "metadata": {},
   "source": [
    "-   This converges to the root $x^* = 235.87$ after 5 iterations (using quite a large stopping tolerance in this case).\n",
    "\n",
    "\n",
    "Consider the NACA0012 aerofoil example with $[x^{(0)}, x^{(1)}] = [0.5, 1]$ using the function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edfdbebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7652489385378323,\n",
       "       converged: True\n",
       "            flag: 'converged'\n",
       "  function_calls: 7\n",
       "      iterations: 6\n",
       "            root: 0.7652489385378323)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def naca0012(x):\n",
    "    # Set the required thickness at the intersection to be 0.1.\n",
    "    t = 0.1\n",
    "\n",
    "    # Evaluate the function.\n",
    "    yp = (\n",
    "        -0.1015 * np.power(x, 4)\n",
    "        + 0.2843 * np.power(x, 3)\n",
    "        - 0.3516 * np.power(x, 2)\n",
    "        - 0.126 * x\n",
    "        + 0.2969 * np.sqrt(x)\n",
    "    )\n",
    "    f = yp - 0.5 * t\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "brentq(naca0012, 0.5, 1.0, xtol=1.0e-4, full_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e345fc",
   "metadata": {},
   "source": [
    "This converges to the root $x^* = 0.765249$ in 6 iterations.\n",
    "\n",
    "Consider the NACA0012 aerofoil example with $[x^{(0)}, x^{(1)}] = [0, 0.5]$ using the function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9d1c85c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.03390094402176156,\n",
       "       converged: True\n",
       "            flag: 'converged'\n",
       "  function_calls: 9\n",
       "      iterations: 8\n",
       "            root: 0.03390094402176156)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brentq(naca0012, 0.0, 0.5, xtol=1.0e-4, full_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bc5991",
   "metadata": {},
   "source": [
    "-   This converges to the other root $x^* = 0.33899$ after 44 iterations.\n",
    "\n",
    "## Summary\n",
    "\n",
    "-   Solving nonlinear equations is hard.\n",
    "\n",
    "-   It is not always possible to guarantee an accurate solution.\n",
    "\n",
    "-   However, it is possible to design a robust algorithm that will usually give a good answer.\n",
    "\n",
    "-   Finding all possible solutions is particularly challenging since we may not know how many solutions there are in advance.\n",
    "\n",
    "## Further reading\n",
    "\n",
    "- `scipy`: Optimization and root finding [`scipy.optimize`](https://docs.scipy.org/doc/scipy/reference/optimize.html)\n",
    "\n",
    "The [slides used in the lecture](./lec18_.ipynb) are also available"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.15.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "source_map": [
   13,
   104,
   110,
   122,
   128,
   134,
   139,
   146,
   160,
   167,
   186,
   192,
   194
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}